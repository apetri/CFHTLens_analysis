%\documentclass[useAMS, usenatbib,usegraphicx,letter]{mn2e}
%\documentclass[11pt]{article}
\documentclass[reprint,aps,prd,superscriptaddress,showkeys,showpacs]{revtex4-1}
\usepackage{epsfig,amsmath,natbib}

\usepackage{aas_macros}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{color}
\usepackage{pbox}

\hypersetup{
	colorlinks=false,
	citecolor=green
}
% \usepackage{graphicx}
% \usepackage{epstopdf}
% \usepackage{natbib}

\begin{document}

\title{CFHTLenS Weak Lensing Emulator and Cosmological Constraints from the Minkowski Functionals and Moments}

\author{Andrea Petri}
\email{apetri@phys.columbia.edu}
\affiliation{Department of Physics, Columbia University, New York, NY 10027, USA}
\affiliation{Physics Department, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Jia Liu}
\affiliation{Department of Astronomy, Columbia University, New York, NY 10027, USA}

\author{Zolt\'an Haiman}
\affiliation{Department of Astronomy, Columbia University, New York, NY 10027, USA}

\author{Morgan May}
\affiliation{Physics Department, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Lam Hui}
\affiliation{Department of Physics, Columbia University, New York, NY 10027, USA}

\author{Jan M. Kratochvil}
\affiliation{Astrophysics and Cosmology Research Unit, University of KwaZulu-Natal, Westville, Durban 4000, South Africa}

\date{\today}

\label{firstpage}

\begin{abstract}
Weak gravitational lensing has been proposed as an interesting probe for cosmology, and in particular for the Dark Energy (DE) equation of state $w$; the fact that typical weak lensing fields are non--Gaussian in nature, quadratic statistics, such as angular power spectra, can miss some of the cosmological information contained in survey data. In this work we examine constraints on the parameter triplet $(\Omega_m,w,\sigma_8)$ from weak lensing Minkowski Functionals and Moments, using the publicly available data from the 154\,deg$^2$ CFHTLenS survey. We utilize a new suite of ray--tracing N-body simulations spanning 91 points in the $(\Omega_m,w,\sigma_8)$ parameter space, replicating the galaxy sky positions, redshift and shape noise read from the CFHTLenS catalogs. We then build an emulator that interpolates the simulated feature spaces, and use it to compute the parameter likelihood, from which we derive the data constraints. We find that dimensionality reduction techniques such as Principal Component Analysis help in stabilizing the constraints with respect to the number of bins used to construct the convergence statistics. Using our full set of statistical descriptors, we constrain the $\Sigma_8=\sigma_8(\Omega_m/0.27)^{0.55}$ to a value of $0.75\pm0.04$ at the $1\sigma$ significance level, well in agreement with the values already quoted in the literature. We find that, while the Minkowski Functionals constraints on the $(\Omega_m,\sigma_8)$ doublet suffer from a marginal bias, the Moments provide a tight unbiased bound on the former parameters, with the main contribution coming from quartic moments of gradients.  

\end{abstract}

%AP: Re-worded the abstract

\keywords{Weak Gravitational Lensing --- Data analysis --- Methods: analytical,numerical,statistical}
\pacs{98.80.-k, 95.36.+x, 95.30.Sf, 98.62.Sb}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%% INTRO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
In this work we use the publicly available CFHTLenS data, that consists of a catalog of $\approx$4.2 million galaxies, combined with a suite of ray-tracing simulations in 91 different cosmological models to constrain the cosmological parameters $\Omega_m$,$\sigma_8$ and the DE equation of state $w$. The paper is organized as follows; we first give an overview of the CFHTLenS catalogs we make use of, summarizing the data reduction techniques we adopted. Next, we give a description of our simulation pipeline, including the procedure to sample the parameter space and a broad description of the ray--tracing algorithm; after this we illustrate the statistical descriptors that we use to extract the cosmological information, complemented with a dimensional reduction and statistical inference framework. We then outline our results, with particular focus on the cosmological parameter constraints. To conclude, we discuss our findings and comment on possible future developments of this analysis.  

%%%%%%%%%%%%%%%%%%%%%%%%%% DATA AND SIMULATIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data and simulations}

\subsection{CFHTLenS data reduction}
\label{cfhtdatareduction}
%
The CFHTLenS survey covers four sky patches of 64,23,44 and 23 deg$^2$ area, for a total of 154 deg$^2$; the publicly released data roughly consist of the creation of a galaxy catalogue using SExtractor \citep{SExtractor}, a photometric redshift estimation with a Bayesian photometric redshift code \citep{PhotoCode} and galaxy shape measurements with \textit{lensfit} \citep{cfht1,cfht2}. The cosmological parameter inferences have been obtained in \citep{CFHTKilbinger} using the two point correlation function (2PCF); additionally a number of authors investigated the CFHTLenS constraining power using statistics that go beyond the usual quadratic ones: \citep{CFHTFu} investigated the effect of using the skewness as an additional probe for cosmology, while \citep{CFHTMasato} investigated CFHT cosmological constraints and systematic errors using the Minkowski Functionals. We apply the following cuts to the galaxy catalogue: mask$<1$, redshift $0.2 < z < 1.3$, fitclass = 0 (which requires the object to be a galaxy) and weight $w>0$ (with larger $w$ indicating smaller shear measurement uncertainty). Applying these cuts leaves us 4.2$\times10^6$ galaxies, 124.7 deg$^2$ residual sky coverage, and average number density $n_{gal} \approx 9.3\,\mathrm{arcmin}^{-2}$. The CFHTLenS galaxy catalogue provides us with the sky position $\pmb{\theta}$, the redshift $z(\pmb{\theta})$ and ellipticity $\mathbf{e}(\pmb{\theta})$ of each galaxy, as well as the individual weight factors $w(\pmb{\theta})$ and additive and multiplicative ellipticity corrections $c(\pmb{\theta}),m(\pmb{\theta})$. Because the CFHTLenS fields are irregularly shaped, we first divide them into 13 squares (subfields) to match the shape and $\approx12$ deg$^2$ size of our simulated maps; these square subfield maps are pixelized according to a Gaussian gridding procedure

%AP: added references and fixed typo. Is there a problem with the galaxy density I reported? (I took it from Jia's paper) 
\begin{equation}
\bar{\mathbf{e}}(\pmb{\theta}) = \frac{\sum_{i=1}^{N_s} W(\vert\pmb{\theta}-\pmb{\theta}_i\vert)w(\pmb{\theta}_i)[\mathbf{e}^{obs}(\pmb{\theta}_i)-c(\pmb{\theta}_i)]}{\sum_{i=1}^{N_s}W(\vert\pmb{\theta}-\pmb{\theta}_i\vert)w(\pmb{\theta}_i)[1+m(\pmb{\theta})]}
\end{equation} 
\begin{equation}
\label{gausskernel}
W_{\theta_G}(\pmb{\theta}) = \frac{1}{2\pi\theta_G^2}\exp{\left(-\frac{\pmb{\theta}^2}{2\theta_G^2}\right)}
\end{equation}
%
where the smoothing scale $\theta_G$ has been fixed to 1.0\,arcmin (and occasionally varied to 1.8 and 3.5\,arcmin for isolated tests); the multiplicative and additive corrections $m,c$ related the observed and true ellipticities of the galaxies in the catalogue
\begin{equation}
\mathbf{e}^{obs} = (1+m)\mathbf{e}^{true} + c
\end{equation}
%
Using the ellipticity grid $\bar{\mathbf{e}}(\pmb{\theta})$ as an estimator for the cosmic shear $\gamma^{1,2}(\pmb{\theta})$, we can perform a non--local Kaiser--Squires inversion to recover the convergence $\kappa(\pmb{\theta})$ from the $E$--mode of the shear field
%
\begin{equation}
\kappa(\mathbf{l}) = \left(\frac{l_1^2-l_2^2}{l_1^2+l_2^2}\right)\gamma^1(\mathbf{l}) + 2\frac{l_1l_2}{l_1^2+l_2^2}\gamma^2(\mathbf{l})
\end{equation}
%
The CFHTLenS catalogues contain masked regions (mainly due to bright stars and incorrect PSF subtraction); these regions with low galaxy number density can induce large errors in the cosmological parameter inferences, hence they need to be masked out. We first create grided maps of the same size and resolution as the $\kappa$ maps, but with each pixel containing the number of galaxies ($n_{gal}$) falling within its window. We then smooth this galaxy surface density map with the same Gaussian window function as equation (\ref{gausskernel}) and we remove regions where $n_{gal} < 5 \,\mathrm{arcmin}^{âˆ’2}$ (see \citep{CFHTMasato}); for a more in--depth description of our data reduction procedure, we refer the reader to our companion paper \citep{Companion}. 

%AP: throughout --> in--depth

\subsection{Simulation design}
In this paragraph we give a description of the method we used to sample the parameter space in our simulation effort. We wish to investigate the non--linear dependency of cosmological probes (in this work Minkowski Functionals and Moments of the $\kappa$ field) on the parameter triplet $\mathbf{p}=(\Omega_m,w,\sigma_8)$, while keeping fixed the other relevant parameters to $(h,\Omega_b,n_s)$ to their fiducial values (0.7,0.046,0.96). We sampled the $D$--dimensional ($D=3$ in this case) parameter space using an irregularly spaced grid, designed with a method similar to \citep{coyote2}; the irregular grid design is necessary, given our limited computing resources: a regular parameter grid with the same average spacing between models would require in fact a prohibitively large number of samples. We limit the parameter sampling in a box of corners $\Omega_m\in[0.07,1],\,w\in[-3.0,0],\,\sigma_8\in[0.1,1.5]$ and we map this sampling box $\Pi$ into an hypercube of unit side; we want to contruct an irregularly spaced grid consisting of $N$ points $\mathbf{x}_i\in[0,1]^D$. Let a \textit{design} $\mathcal{D}$ be the set of this irregularly spaced $N$ points: we wish to find an optimal design, in which the points are spread as uniformly as possible inside the box. Following \citep{coyote2}, we choose our optimal design as the minimum of the cost function

%AP: Justified irregular parameter sampling

\begin{equation}
\label{costfunction}
d(\mathcal{D}) = \frac{2D^{1/2}}{N(N-1)}\sum_{i<j}^N\frac{1}{\vert\mathbf{x}_i-\mathbf{x}_j\vert}
\end{equation} 
%
This problem is mathematically equivalent to the minimization of the Coulomb potential energy of $N$ unit charges in a unit box, which will make sure that the charges are as evenly spread as possible throughout the confining volume. Finding the optimal design $\mathcal{D}_m$ that minimized (\ref{costfunction}) can be computationally very demanding, and hence we decided to use a simplified approach that, although approximate, serves our purposes for the grid design. We use an iterative procedure:
\begin{enumerate}
\item We start from the diagonal design $\mathcal{D}_0$: $x_i^d\equiv i/(N-1)$
\item We shuffle the coordinates of the particles in each dimension independently $x_i^d = \mathcal{P}_d\left(\frac{1}{N-1},\frac{2}{N-1},...,1\right)$ where $\mathcal{P}_1,...,\mathcal{P}_D$ are random independent permutations of $(1,2,...,N)$
\item We pick a random particle pair $(i,j)$ and a random coordinate $d\in\{1,...,D\}$ and swap $x_i^d\leftrightarrow x_j^d$
\item We compute the new cost function, if this is less than the previous step, we keep the exchange, otherwise we revert the coordinate swap
\item We repeat steps 3 and 4 until the relative cost function change is less than a chosen accuracy parameter $\epsilon$ 
\end{enumerate}
%
We found that for $N=91$ grid points, order of $10^5$ iterations are sufficient to reach an accuracy of $\epsilon\sim10^{-4}$; once the optimal design $\mathcal{D}_m$ has been found, we can invert the mapping $\Pi\rightarrow[0,1]^3$ to find our simulation parameter sampling $\mathbf{p}_s$, which we show in Table \ref{designtable} and Figure \ref{designfig}.
%
\begin{table*}
\begin{tabular}{c|ccc||c|ccc||c|ccc||c|ccc}
$N$ & $\Omega_m$ & $w$ & $\sigma_8$ & $N$ & $\Omega_m$ & $w$ & $\sigma_8$ & $N$ & $\Omega_m$ & $w$ & $\sigma_8$ & $N$ & $\Omega_m$ & $w$ & $\sigma_8$ \\ \hline
1 & 0.136 & -2.484 & 1.034 & 26 & 0.380 & -2.424 & 0.199 & 51 & 0.615 & -1.668 & 0.185 & 76 & 0.849 & -0.183 & 0.821 \\
2 & 0.145 & -2.211 & 1.303 & 27 & 0.389 & -0.939 & 0.454 & 52 & 0.624 & -2.757 & 0.327 & 77 & 0.859 & -1.182 & 1.415 \\
3 & 0.155 & -0.393 & 0.652 & 28 & 0.399 & -1.938 & 1.500 & 53 & 0.634 & -1.575 & 0.976 & 78 & 0.869 & -2.031 & 0.227 \\
4 & 0.164 & -2.181 & 0.313 & 29 & 0.409 & -2.940 & 0.737 & 54 & 0.643 & -2.454 & 1.444 & 79 & 0.878 & -2.697 & 0.524 \\
5 & 0.173 & -0.423 & 1.231 & 30 & 0.418 & -1.758 & 0.383 & 55 & 0.652 & -1.029 & 1.458 & 80 & 0.887 & -0.363 & 0.439 \\
6 & 0.183 & -0.909 & 0.269 & 31 & 0.427 & -2.910 & 0.411 & 56 & 0.661 & -0.486 & 0.892 & 81 & 0.897 & -0.999 & 0.468 \\
7 & 0.192 & -1.605 & 1.401 & 32 & 0.436 & -0.060 & 0.878 & 57 & 0.671 & -2.364 & 0.793 & 82 & 0.906 & -1.698 & 1.273 \\
8 & 0.201 & -2.787 & 0.807 & 33 & 0.446 & -1.212 & 1.486 & 58 & 0.681 & -2.970 & 0.610 & 83 & 0.915 & -2.544 & 1.175 \\
9 & 0.211 & -0.333 & 0.341 & 34 & 0.455 & -2.637 & 1.373 & 59 & 0.690 & -1.332 & 0.482 & 84 & 0.925 & -0.636 & 1.259 \\
10 & 0.221 & -1.485 & 0.666 & 35 & 0.464 & -2.121 & 0.906 & 60 & 0.700 & -0.273 & 0.283 & 85 & 0.943 & -2.394 & 0.835 \\
11 & 0.239 & -1.848 & 0.962 & 36 & 0.474 & -1.302 & 0.114 & 61 & 0.709 & -2.061 & 0.425 & 86 & 0.953 & -1.545 & 0.355 \\
12 & 0.249 & -2.727 & 0.369 & 37 & 0.483 & -1.515 & 0.680 & 62 & 0.718 & -1.728 & 1.472 & 87 & 0.963 & -2.151 & 0.510 \\
13 & 0.258 & -1.395 & 0.241 & 38 & 0.493 & -0.243 & 0.297 & 63 & 0.728 & -0.120 & 0.596 & 88 & 0.972 & -0.666 & 0.694 \\
14 & 0.267 & -2.667 & 1.317 & 39 & 0.502 & -1.152 & 1.189 & 64 & 0.737 & -2.847 & 1.203 & 89 & 0.981 & -1.242 & 1.048 \\
15 & 0.276 & -0.849 & 1.429 & 40 & 0.512 & -0.819 & 0.849 & 65 & 0.746 & -0.090 & 1.118 & 90 & 0.991 & -1.908 & 1.020 \\
16 & 0.286 & -1.272 & 1.104 & 41 & 0.521 & -2.334 & 0.538 & 66 & 0.755 & -0.456 & 1.359 & 91 & 1.000 & -1.425 & 0.708 \\
17 & 0.295 & -1.878 & 0.100 & 42 & 0.530 & 0.000 & 0.624 & 67 & 0.765 & -2.091 & 1.076 & -- & -- & -- & -- \\
18 & 0.305 & -0.879 & 0.765 & 43 & 0.540 & -0.030 & 1.161 & 68 & 0.775 & -1.122 & 1.132 & -- & -- & -- & -- \\
19 & 0.315 & -2.241 & 0.638 & 44 & 0.549 & -1.818 & 1.287 & 69 & 0.784 & -1.062 & 0.779 & -- & -- & -- & -- \\
20 & 0.324 & -2.001 & 1.217 & 45 & 0.558 & -2.577 & 1.146 & 70 & 0.794 & -1.365 & 0.156 & -- & -- & -- & -- \\
21 & 0.333 & -0.213 & 0.552 & 46 & 0.568 & -0.516 & 1.331 & 71 & 0.803 & -2.607 & 0.255 & -- & -- & -- & -- \\
22 & 0.342 & -2.817 & 1.062 & 47 & 0.577 & -3.000 & 0.948 & 72 & 0.812 & -1.788 & 0.722 & -- & -- & -- & -- \\
23 & 0.352 & -0.576 & 1.090 & 48 & 0.587 & -2.304 & 0.128 & 73 & 0.821 & -2.880 & 0.863 & -- & -- & -- & -- \\
24 & 0.361 & -0.606 & 0.171 & 49 & 0.596 & -0.696 & 0.496 & 74 & 0.831 & -0.759 & 0.213 & -- & -- & -- & -- \\
25 & 0.370 & -0.303 & 1.345 & 50 & 0.606 & -0.789 & 0.142 & 75 & 0.840 & -2.274 & 1.387 & -- & -- & -- & -- \\
\end{tabular}
\caption{List of the CFHTemu1 grid points in parameter space}
\label{designtable}
\end{table*}
%
\begin{figure*}
\begin{center}
\includegraphics[scale=0.4]{Figures/design.eps}
\caption{$(\Omega_m,w)$ and $(\Omega_m,\sigma_8)$ projections of our the simulation design; the blue points correspond to the CFHTemu1 simulation set, which consists of one $N$--body simulation per point, while the red point corresponds to the CFHTcov simulation set, which is based on 50 independent $N$--body simulations}
\label{designfig}
\end{center}
\end{figure*}
%
For each parameter point on the grid $\mathbf{p}_s$ we run one $N$--body simulation and perform ray tracing through it, as described in \S~\ref{raysim}, to simulate CFHTLenS shear catalogs; this set of simulations will be called CFHTemu1 throughout the rest of this work. Additionally, we run 50 independent $N$--body simulations with a \textit{fiducial} parameter choice $\mathbf{p}_0=(0.26,-1.0,0.8)$ for the purpose of measuring accurately the covariance matrices which will serve the parameter inferences as in \S-\ref{cosmostats}; this set of simulations will be called CFHTcov throughout the rest of this work.    

\subsection{Ray Tracing Simulations}
\label{raysim}
The goal of this paragraph is to give an outline of our simulation pipeline; the fluctuations in the dark matter density field between a source at redshift $z$ and an observer located on Earth will cause small deflections to the trajectories of light rays travelling from the source to the observer. The fluctuations in the dark matter field are described by their gravitational potential $\Phi(\mathbf{x},z)=\Phi(\mathbf{x}_\perp,\chi(z))$, where we can trade the physical coordinates $\mathrm{x}$ with the comoving distance from the observer $\chi$ and two transverse coordinates $\mathbf{x}_\perp=\chi\pmb{\beta}$ using the flat sky approximation. Here $\pmb{\beta}$ refers to the angular coordinate on the sky of a physical point $\mathbf{x}$, as seen from the observer. We estimate the dark matter gravitational potential running $N$--body simulations (with $N=512^3$) with the public code Gadget2 \citep{Gadget2}, using a comoving box size of $240h^{-1}$Mpc. Using a similar procedure as in \citep{RayTracingJain,RayTracingHartlap}, the equation that governs the light ray deflections can be written in the form
\begin{equation}
\label{raytrajectory}
\frac{d^2\mathbf{x}(\chi)}{d\chi^2} = -\frac{2}{c^2}\nabla_{\mathbf{x}_\perp}\Phi(\mathbf{x}_\perp(\chi),\chi)
\end{equation}
%
where $\mathbf{x}(\chi)$ is the trajectory of a single light ray. Suppose that a light ray reaches the observer at an angular position $\pmb{\theta}$ on the sky: we want to know where this light ray originated, knowing it comes from a redshift $z_s$. To answer this question we need to integrate equation (\ref{raytrajectory}) with the initial conditions $\pmb{\beta}(0;\pmb{\theta})=\pmb{\theta}$, $\dot{\pmb{\beta}}(0;\pmb{\theta})=0$ up to a distance $w_s=w(z_s)$ to obtain the source angular position $\pmb{\beta}(w_s;\pmb{\theta})$; for the light ray trajectory solver, based on equation (\ref{raytrajectory}), we use our proprietary implementation Inspector Gadget. Once we know the details of the light ray trajectories, we can easily infer the weak lensing interesting quantities by taking the angular derivatives of the ray deflections $A(\chi_s;\pmb{\theta}) = \partial \pmb{\beta}(\chi_s;\pmb{\theta})/\partial\pmb{\theta}$ and performing the usual spin decomposition to infer the convergence $\kappa$ and the shear components $(\gamma^1,\gamma^2)$
%
\begin{equation}
A(\chi_s;\pmb{\theta}) = (1-\kappa(\chi_s;\pmb{\theta}))\pmb{I} - \gamma^1(\chi_s;\pmb{\theta})\sigma^3 - \gamma^2(\chi_s;\pmb{\theta})\sigma^1
\end{equation}  
%
where $\pmb{I}$ is the $2\times2$ identity and $\sigma^{1,3}$ are the first and third Pauli matrices. $\kappa$ is related to the source apparent magnification, while $(\gamma^1,\gamma^2)$ are related to the source apparent ellipticity, as seen from the observer. Given a source with intrinsic ellipticity $\mathbf{e}_s=e^1_s + ie^2_s$, its observed ellipticity as seen from an observer will be modified by the cosmic shear $\pmb{\gamma}=\gamma^1 + i\gamma^2$ following
%
\begin{equation}
\mathbf{e} = 
\begin{cases}
\frac{\mathbf{e}_s+\mathbf{g}}{1+\mathbf{g}^*\mathbf{e}_s} \,\,\,\,\,\,\,\, \vert \mathbf{g}\vert \leq 1 \\ \\
\frac{1+\mathbf{ge}_s^*}{\mathbf{e}_s^* + \mathbf{g}^*} \,\,\,\,\,\,\,\, \vert \mathbf{g}\vert > 1
\end{cases}
\end{equation}
%
where $\mathbf{g} = \pmb{\gamma}/1-\kappa$ is the reduced shear. For each simulated galaxy, we assign an intrinsic ellipticity by rotating the observed ellipticity for that galaxy by a random angle on the sky, while conserving its magnitude $\vert\mathbf{e}\vert$. To be consistent with the CFHTLenS analysis, we adopt the weak lensing limit ($\vert\pmb{\gamma}\vert\ll1,\kappa\ll1$),hence $\mathbf{g}\approx\pmb{\gamma}$ and $\mathbf{e}\approx \mathbf{e}_s+\pmb{\gamma}$. We also add the multiplicative shear corrections by replacing $\pmb{\gamma}$ with $(1+m)\pmb{\gamma}$. We note that the observed ellipticity for a particular galaxy already contains information from the lensing from Large Scale Structure, but the random angle rotation makes this contribution second order in $\kappa$: consistently with the weak lensing approximation, the lensing singal from the simulations is first order in $\kappa$ and hence the randomly rotated observed ellipticities can be effectively considered as intrinsic ellipticities. Like for the CFHTLenS data, we proceed in constructing the simulated $\kappa$ maps as explained in \S\ref{cfhtdatareduction}. These final simulation products are then processed together with the $\kappa$ maps obtained from the data to calculate the confidence intervals on the parameter triplet $(\Omega_m,w,\sigma_8)$.

%AP: changed comoving distance symbol to chi; knocked out the ray tracing figure; clarified the signal double counting issue

%%%%%%%%%%%%%%%%%%%%%%%%%% METHODS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical methods}
The goal of this section is to describe the framework in which we combine the CFHT data and our simulations in order to derive the constraints on the cosmological parameter triplet $(\Omega_m,w,\sigma_8)$; we measure a set of statistical descriptors from the data and the simulations, which will then be compared in a bayesian fashion in order to compute parameter confidence intervals.

\subsection{Descriptors}
The statistical descriptors we consider on this work are the Minkowski Functionals (MFs) and the low order moments (LM) of the convergence field. The three MFs $(V_0,V_1,V_2)$ are topological descriptors of the convergence random field $\kappa(\pmb{\theta})$, which probe respectively the area, perimeter and genus characteristic of the $\kappa$ excursion sets $\Sigma_{\kappa_0}$, defined as $\Sigma_{\kappa_0}=\{\kappa>\kappa_0\}$. Following \citep{Petri2013,MinkJan} we use the following local estimators to measure the MFs from the $\kappa$ images. 
%
\begin{equation*}
\label{v0meas}
V_0(\kappa_0)=\frac{1}{A}\int_A\Theta(\kappa(\pmb{\theta})-\kappa_0)d\pmb{\theta},
\end{equation*}
\begin{equation}
\label{v1meas}
V_1(\kappa_0)=\frac{1}{4A}\int_A\delta(\kappa(\pmb{\theta})-\kappa_0)\sqrt{\kappa_x^2+\kappa_y^2}d\pmb{\theta},
\end{equation}
\begin{equation*}
\label{v2meas}
V_2(\kappa_0)=\frac{1}{2\pi A}\int_A\delta(\kappa(\pmb{\theta})-\kappa_0)\frac{2\kappa_x\kappa_y\kappa_{xy}-\kappa_x^2\kappa_{yy}-\kappa_y^2\kappa_{xx}}{\kappa_x^2+\kappa_y^2}d\pmb{\theta}.
\end{equation*}
%
Where $A$ is the total area of the fields of view we have been using and the notation $\kappa_{x,y}$ indicates gradients of the $\kappa$ field, which we evaluate using finite differences. The first Minkowski functional, $V_0$, is equivalent to the cumulative one--point PDF of the $\kappa$ field, $\partial V_0$ (which can be obtained by differentiation $\partial V_0(\kappa_0)=dV_0(\kappa_0)/d\kappa_0$), while $V_1,V_2$ are sensitive to the correlations between nearby pixels. In addition to these topological descriptors, we consider a set of low order moments (LM) of the convergence field (two quadratic, three cubic and four quartic), which are defined in the following way
%
\begin{equation}
\label{momentestimator}
\begin{matrix}
\mathrm{LM_2}: \sigma_{0,1}^2 = \langle\kappa^2\rangle,\langle\vert\nabla\kappa\vert^2\rangle, \\ \\
\mathrm{LM_3}: S_{0,1,2} = \langle\kappa^3\rangle,\langle\kappa\vert\nabla\kappa\vert^2\rangle,\langle\kappa^2\nabla^2\kappa\rangle, \\ \\
\mathrm{LM_4}: K_{0,1,2,3} = \langle\kappa^4\rangle,\langle\kappa^2\vert\nabla\kappa\vert^2\rangle,\langle\kappa^3\nabla^2\kappa\rangle,\langle\vert\nabla\kappa\vert^4\rangle.
\end{matrix}
\end{equation}
%
If the $\kappa$ field was Gaussian, one could express all the Minkowski Functionals in terms of the LM2 moments, as expected from the fact that, for a Gaussian field, the only meaningful statistics are the quadratic ones. In reality, weak lensing convergence fields, as the CFHTLenS ones, are non--Gaussian and the MF and LM descriptors are in principle equivalent. \citep{Munshi12,Matsubara10} studied a perturbative expansion of the MF descriptors in powers of the field standard deviation $\sigma_0$, which, when truncated at order $O(\sigma_0^2)$, can be expressed completely in terms of the LM up to quartic order. Such perturbative series, however, have been shown not to converge in \citep{Petri2013} unless the weak lensing fields are smoothed with windows of size comparable to $\sim 15^\prime$. Because of this, throughout this work, we treat MF and LM as separate statistical descriptors. In addition to these two descriptors, we consider the $\kappa$ angular power spectrum $P_l$ defined as
\begin{equation}
\label{powerspectrum}
\langle\tilde{\kappa}(\mathbf{l})\tilde{\kappa}(\mathbf{l}')\rangle=(2\pi)^2\delta_D(\mathbf{l}+\mathbf{l}')P_l
\end{equation}  
%
%AP: independent --> separate
%
where $\tilde{\kappa}(\mathbf{l})$ is the Fourier transform of the $\kappa$ field and $\delta_D$ is the usual Dirac delta function; while the use of this statistic is not a novelty, we want indeed to compare the results we get using MF and LM to the ones already present in the literature. A summary of the statistical descriptors we used is presented in Table \ref{desctable}. 
%
\begin{table}
\begin{tabular}{c|c|c} \hline
Descriptor & Details & $N_b$ \\ \hline
$V_0,V_1,V_2$ (MF) & $\kappa_0\in[-0.04,0.12]$ & 50 \\
Power Spectrum (PS) & $l \in [300,5000]$ & 50 \\
Moments (LM) & -- & 9 \\
\end{tabular}
\caption{Summary of the descriptors we used, along with the specifications and the number of bins $N_b$ used}
\label{desctable}
\end{table}
%
When measuring statistical features on $\kappa$ maps, particular attention must be given to the effect of masked pixels: this does not pose a particular problem for the MFs and LM statistics, since the estimators in (\ref{v1meas}) and (\ref{momentestimator}) are well defined over all the non--masked region, with exception of the few pixels that are close to the mask boundaries. Masking effects on Minkowski Functionals constraints with CFHTLenS have been studies extensively by \citep{CFHTMasato}. The situation is more complicated for the Power Spectrum measurements, that require the evaluations of Fourier Transforms and hence rely on the value of each pixel on the map. Although sophisticated interpolation approaches over the masked regions have been studied (see for example \citep{VplasInterpolation}), for the sake of simplicity we put a value of 0 in each masked pixel, arguing that, given the typical angular size of the masked regions we consider, this will have little effect on the Power Spectrum at the range of multipoles in Table \ref{desctable}. We nevertheless believe that the way we deal with masked sky regions is very robust, because we apply the same masks to our simulations. 

%AP: Stressed the fact that we apply the same mask to the simulations; added reference to Yoshida/Shirasaki 

\subsection{Cosmological parameter inferences}
\label{cosmostats}
In this paragraph we give a brief outline of the statistical framework we adopted for computing the cosmological parameter confidence levels from the CFHTLenS observations; we make use of the MF and LM statistical descriptors outlined in the previous paragraph. We refer to $M_i^r(\mathbf{p})$ as the measured descriptor from a realization $r$ of one of our simulations with a choice of cosmological parameters $\mathbf{p}$, and to $D_i$ as the measured descriptor from the CFHTLenS data. In this notation, $i$ is an index that refers to the particular bin on which the descriptor is evaluated (for example $i$ can range from 0 to 9 for the LM statistic and from 0 to $N_b-1$ for a Minkowski Functional measured on $N_b$ different excursion sets). Once we make an assumption for the data likelihood $\mathcal{L}_d(D_i\vert \mathbf{p})$ and for the parameter priors $\Pi(\mathbf{p})$, we can use the Bayes theorem to compute the parameter likelihood $\mathcal{L}_p$ as follows

\begin{equation}
\label{parameterlikelihood}
\mathcal{L}_p(\mathbf{p}\vert D_i) = \frac{\mathcal{L}_d(D_i\vert \mathbf{p})\Pi(\mathbf{p})}{N_{\mathcal{L}}}
\end{equation}
%
where $N_{\mathcal{L}}$ is a $\mathbf{p}$--independent constant that ensures the proper normalization for $\mathcal{L}_p$; we make the usual assumption that the data likelihood $\mathcal{L}_d(D_i\vert \mathbf{p})$ is a Gaussian

%AP: what kind of quantitative test should I implement? We can use Anderson-Darling to check how gaussian the pdf in each bin is, but this doesn't tell anything on the bin correlations. Also which bins shall I choose? There is a problem if the likelihood is non-Gaussian: we don't know what to do. Basically this is the only functional form with which we can model the PDF and bin correlations analytically, and compute the parameter likelihood semi-analytically, otherwise we need to dig into more sophisticated non--parametric methods in which I am not that proficient yet. 

\begin{equation}
\label{datalikelihood}
\begin{matrix}
\mathcal{L}_d(D_i\vert \mathbf{p}) = ((2\pi)^{N_b}\det{\mathbf{C}})^{-1/2} e^{-\frac{1}{2}\chi^2(D_i\vert \mathbf{p})} \\ \\
\chi^2(D_i\vert \mathbf{p}) = \mathbf{(D - M(p))C^{-1}(D-M(p))}
\end{matrix}
\end{equation} 
%
The simulated descriptors $\mathbf{M(p)}$ are measured from an average over realizations
\begin{equation}
M_i(\mathbf{p}) = \frac{1}{R}\sum_{r=1}^R M_i^r 
\end{equation}
%
and the covariance matrix 
\begin{equation}
\label{datacov}
C_{ij} = \frac{1}{R-1} \sum_{r=1}^R [M_i^r(\mathbf{p}_0)-M_i(\mathbf{p}_0)][M_j^r(\mathbf{p}_0)-M_j(\mathbf{p}_0)]
\end{equation}
%
is measured from a simulation set based on 50 independent $N$--body simulations with parameters $\mathbf{p}_0=(0.26,-1.0,0.8)$ and is assumed to be model--independent; because of this the normalization constant in (\ref{datalikelihood}) is also model--independent. When computing parameter constraints from CFHTLenS weak lensing data alone, we make a flat prior assumption for $\Pi(\mathbf{p})$. Parameter inferences are made estimating the location of the maximum of the parameter likelihood in (\ref{parameterlikelihood}), which we call $\mathbf{p}_{ML}(D_i)$ as well as its confidence contours. A $N\sigma$--confidence contour of $\mathcal{L}_p(\mathbf{p}\vert D_i)$ is defined to be the subset of points in parameter space on which the likelihood has a constant value $c_N$ and 
\begin{equation}
\label{ennesigma}
\int_{\mathcal{L}>c_N} \mathcal{L}_p(\mathbf{p}\vert D_i) d\mathbf{p} = \frac{1}{\sqrt{2\pi}}\int_{-N}^N dx e^{-x^2/2}
\end{equation}
%
Given the low dimensionality of the parameter space we consider $(N_p=3)$ we are able to evaluate the parameter likelihood (\ref{parameterlikelihood}) on a finely spaced $100\times100\times100$ mesh within the prior window $\Pi(\mathbf{p})$, and hence we are able to evaluate the likelihood maximum $\mathbf{p}_{ML}(D_i)$ and the contour levels $c_N$ directly without the need to use more sophisticated MCMC methods. We know how to evaluate the data likelihood (\ref{datalikelihood}) on the simulation grid $\mathbf{p}_s$, but more work needs to be done to interpolate $M_(\mathbf{p})$ at an arbitrary intermediate point; we decided to use a Radial Basis Function (RBF) interpolation scheme. We approximate the model descriptor as
\begin{equation}
M(\mathbf{p}) = \sum_{s=1}^N \lambda_s\phi(\vert\mathbf{p}-\mathbf{p}_s\vert)
\end{equation}
%
where $\phi$ has been chosen as a multiquadric function $\phi(r)=\sqrt{1+(r/r_0)^2}$ with $r_0$ chosen as the mean euclidean distance between the points in the simulated grid $\mathbf{p}_s$. The constant coefficients $\lambda_s$ can be determined imposing the $N$ constraints $M(\mathbf{p}=\mathbf{p}_s)=M(\mathbf{p_s})$, which enforce the fact that the interpolation should be exact at the simulated points. The interpolation computations are conveniently performed using the Scipy library \citep{scipy}. We studied the accuracy of emulator, built with the CFHTemu1 simulations, by interpolating the convergence features to the fiducial parameter setting $(\Omega_m,w,\sigma_8)=(0.26,-1.0,0.8)$ and comparing the result to the one expected from the CFHTcov simulations. Figure \ref{emulatorAccuracy} shows that our Power Spectrum emulator is at least 10\% accurate for the lower multipoles, and this accuracy can even be comparable with 1\% for the higher multipoles. The Minkowski Functionals emulator has a good, $\lesssim$10\% for the first 30 bins (which correspond to $\kappa$ values in $[-0.04,0.08]$) and deteriorates with numerical noise for the remaining 20 bins; we take care of these inaccuracies in our dimensionality reduction framework, which we explain in the next paragraph. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.45]{Figures/emulator_accuracy.eps}
\end{center}
\caption{Accuracy of the emulator based on the CFHTemu1 simulations: we show the difference between the interpolated feature at the fiducial parameter setting and the expected feature from the CFHTcov simulations, in units of the standard deviation in each bin $i$ (determined from the diagonal elements of the CFHTcov covariance matrix). We show the accuracy results for the Power Spectrum (red) and Minkowski Functionals $V_0$ (green), $V_1$ (blue) and $V_2$ (black).}
\label{emulatorAccuracy}
\end{figure}  

\subsection{Dimensionality reduction}
\label{pcasection}
The main goal of this work is constraining the cosmological parameter triplet $(\Omega_m,w,\sigma_8)$ using the CFHTLenS data; when the question on the accuracy of the $N\sigma$ contours is raised, particular attention should be given on the effect of binning choices on contour sizes computed with equations (\ref{parameterlikelihood})--(\ref{ennesigma}). It is known that the choice of the number of bins, $N_b$, can have a non--negligible effect on the contour sizes (see \citep{Petri2013} for an example with simulated datasets); in order for our results to be robust under this effect, we adopt a Principal Component Analysis (PCA) approach. Our main physical motivation for this approach is that, albeit we need to specify $N_b$ numbers in order to fully characterize a binned descriptor, we suspect that the majority of the constraining information (of a particular feature) is contained in a fixed limited number of linear combinations of its binning.  In the framework adopted by \citep{coyote2}, for example, the authors find that the majority of the cosmological information in the matter Power Spectrum is contained in only 5 selected linear combinations of the multipoles. Because of this, we believe that dimensionality reduction techniques such as PCA can help in delivering constraints that are independent on the number of bins $N_b$ that we choose originally. In order to compute the Principal Components of our feature space, we consider the CFHTemu1 simulations, which sample the cosmological parameter space at the $N=91$ points outlined in Table \ref{designtable} and allow us to compute the $N\times N_b$ model matrix $M_{pi}=M_i(\mathbf{p})$. Following a standard procedure (see \citep{astroMLText} for example), we then derive the whitened model matrix $\tilde{M}_{pi}$, defined by subtracting the mean in each bin, and normalizing it by its variance; next we proceed in a SVD decomposition of $\mathbf{\tilde{M}}$
\begin{equation}
\label{svd}
\mathbf{U}\Sigma \mathbf{V}^T=\frac{\mathbf{\tilde{M}}}{\sqrt{N-1}}
\end{equation}   
%
where $\Sigma_{ij}=\Sigma_i\delta_{ij}$ is a diagonal matrix and $V^T_{ij}$ is the $i$--th principal component of $\mathbf{\tilde{M}}$, with the index $j$ ranging from $0$ to $N_b-1$. To rank the Principal Components $V^T$ in order of importance, we note that the diagonal matrix $\Sigma^2$ is nothing more than the diagonalization of the model covariance (not to be confused with the feature covariance in (\ref{datacov}))
\begin{equation}
\frac{1}{N-1}\mathbf{\tilde{M}}^T\mathbf{\tilde{M}} = \mathbf{V}\Sigma^2\mathbf{V}^T
\end{equation} 
%
We follow the standard interpretation of PCA components, stating that the only meaningful components $V^T_i$ in the analysis (i.e. the ones that contain the relevant cosmological information) are the ones that correspond to the biggest eigenvalues $\Sigma^2_{i}$, with the smallest eigenvalues corresponding to noisy patterns in the model, due to numerical inaccuracies in the simulation pipeline. We expect our constraints to be stable with respect to the number of components, once enough components have been included. Using the fact that different Principal Components are orthogonal, we perform a PCA projection on our feature space by whitening the features and computing the dot product with the principal components, keeping only the first $n$ components
\begin{equation}
\label{pcaprojection}
M(n)_{i}^r = V^T(n)_{ij}\tilde{M}_j^r \,\,\,\, ; \,\,\,\,  D(n)_i = V^T(n)_{ij}\tilde{D}_j
\end{equation}
%
where we indicate with $V^T(n)$ the truncation of $V^T$ to the first $n$ rows. The dimensionality reduction comes from the fact that we expect most of the cosmological information to be contained in a number of components $n<N_{b}$. Looking at the dimensionality issue from a more geometrical perspective, we expect our feature space to be a 3 dimensional manifold embedded in a $N_b$--dimensional space. The dimensionality reduction problem is equivalent to the accurate reconstruction of the coordinate chart of this feature manifold. As outlined in \citep{astroMLText}, the coordinate chart constructed with the PCA projection in (\ref{pcaprojection}) is accurate for reasonably flat feature manifolds: when curvature effects become important, more advanced projection techniques (such as Locally Linear Embedding) have to be employed. We are allowed flexibility on this flatness assumption because we allow the number of principal components to be bigger than 3. 

%AP: Addressed dimensionality issues 

%%%%%%%%%%%%%%%%%%%%%%%%%% RESULTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
\begin{figure*}
\includegraphics[scale=0.4]{Figures/pca_components.eps}
\caption{Principal Components of the Power Spectrum(red), $V_0$ (blue), $V_1$ (green), $V_2$ (black) and the Moments (orange) feature spaces; the left plot shows the magnitudes of the PCA eigenvalues $\Sigma_i^2$, the right plot shows their cumulative sum. A dashed black line has been drawn in correspondence of $n=3$ components}
\label{pcafig}
\end{figure*}
%
\begin{figure*}
\includegraphics[scale=0.4]{Figures/robustness_pca_Omega_m-sigma8.eps}
\caption{PCA projection dependence of the $1\sigma$ contours in the $(\Omega_m,\sigma_8)$ plane obtained from a mock observation constructed with the CFHTcov simulations; the different panels refer to the descriptors (from left to right, top to bottom) $V_0$, $\partial V_0$(PDF), $V_1$, $V_2$, Power Spectrum and Moments}
\label{robustnessfig}
\end{figure*}
%

\section{Results}
\label{results}
This section describes the results we have obtained in this work, and is organized as follows: we first use our simulations to perform a robustness analysis of the parameter confidence intervals with respect to the number of PCA components used in the projection. We then show the cosmological constraints from the CFHTLenS data. Because of the relatively small size of this survey, degeneracy in the parameters can have undesired effect on the constraints: this is why, in addition to the usual $(\Omega_m,w,\sigma_8)$ parametrization, we consider a slightly different one, built with the parameter triplet $(\Omega_m,w,\Sigma_8)$ with $\Sigma_8(\alpha)=\sigma_8(\Omega_m/0.27)^\alpha$. The reason for this is that, while $\Omega_m$ and $\sigma_8$ are poorly constrained by CFHTLenS due to degeneracies, the $\Sigma_8(\alpha)$ combination lies on the smallest variance direction of $\mathcal{L}(\Omega_m,\sigma_8)$ for a suitable choice of $\alpha$, and hence has a much smaller relative uncertainty. We can derive this optimal value of $\alpha$ from the full three dimensional likelihood $\mathcal{L}(\Omega_m,w,\sigma_8)$, from which we can compute the expectation values
\begin{equation}
\mathds{E}(\alpha) = \langle\Sigma_8(\alpha)\rangle \,\,\, ; \,\,\, \mathds{V}(\alpha) = \langle(\Sigma_8(\alpha)-\mathds{E}(\alpha))^2\rangle
\end{equation}
%
and minimizing the ratio $\mathds{E}/\sqrt{\mathds{V}}$ with respect to $\alpha$. This procedure yields a value $\alpha\approx0.55$ for the statistical descriptors that we consider, consistent to what can be found in the literature. We show the constraints in both the $(\Omega_m,\sigma_8)$ and $(w,\Sigma_8)$ planes; we also show the $\Sigma_8$ marginalized likelihood. We finally study the effect of combining different descriptors, in hope that this helps in tightening the constraints. A summary with the complete set of results, along with the relevant Figures, is shown in Table \ref{summarytable}. 

%AP: Justified value of alpha=0.55

\subsection{Robustness}
%
The purpose of this paragraph is to convince the reader that the cosmological constraints we show in this paper are numerically robust, i.e. they are reasonably stable once we consider an large enough number $n$ of Principal Components. Figure \ref{pcafig} shows the PCA eigenvalues that result from the SVD decomposition of our feature spaces, as well as the cumulative sum of these eigenvalues, normalized to 1. Figure \ref{robustnessfig} shows the dependence of the $(\Omega_m,\sigma_8)$ constraints on the number of principal components $n$, and clearly indicates that we only need a limited number of components in order to capture the cosmological information contained in our descriptors. The number of components to keep, however, depends on the particular descriptor considered. 

%AP: Added inline description of the figures

\subsection{Cosmological constraints}
%
We make use of equations (\ref{parameterlikelihood})-(\ref{ennesigma}) to compute the $1\sigma$ constraints on different combinations of cosmological parameters: in Figures \ref{contours3single},\ref{contoursMoments} we show the constraints in the $(\Omega_m,\sigma_8)$ plane, in Figure \ref{contours3singleRep} we focus on the $(w,\Sigma_8)$ plane. Additionally we show the $\Sigma_8$ likelihood function (marginalized over $\Omega_m$ and $w$) in Figure \ref{likelihoodSi8single}. We discuss these results in \S~\ref{discussion}. 

\begin{figure*}
\begin{center}
\includegraphics[scale=0.45]{Figures/contoursOmega_m-sigma8_single.eps}
\includegraphics[scale=0.45]{Figures/contoursmockOmega_m-sigma8_single.eps}
\end{center}
\caption{$1\sigma$ constraints on the $(\Omega_m,\sigma_8)$ parameter doublet using the Power Spectrum (red), $V_0$ (blue), $V_1$ (green), $V_2$ (black) and Moments (orange) statistics; we show the constraints from the data (left panel) and from a mock observation constructed using the CFHTcov simulations as data (right panel). The contours are calculated from the parameter likelihood function $\mathcal{L}$ marginalized over $w$. The parentheses near the descriptor label refer to the number of principal components.}
\label{contours3single}
\end{figure*}

\begin{figure}
\begin{center}
\includegraphics[scale=0.45]{Figures/contours_momentsOmega_m-sigma8.eps}
\end{center}
\caption{$1\sigma$ constraints on the $(\Omega_m,\sigma_8)$ parameter doublet using the Moments, with different colors corresponding to different moment combinations: we show the results obtained using the one--point moments $\sigma_0^2,S_0,K_0$ (black) and using the one--point moments combined at $(1.0,3.0,5.8)$\,arcmin smoothing (gray scale). We also show the improvements that we get when we add the quadratic (green), cubic (red) and quartic (moments) of gradients. The yellow contour refers to the whole set of 9 Moments}
\label{contoursMoments}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.45]{Figures/contoursw-Sigma8Om055_single.eps}
\end{center}
\caption{$1\sigma$ constraints on the $(w,\Sigma_8)$ parameter doublet from the CFHTLenS data, obtained with the Power Spectrum (red), $V_0$ (blue), $V_1$ (green), $V_2$ (black) and Moments (orange) statistics; we show the constraints from the data. The contours are calculated from the parameter likelihood function $\mathcal{L}$ marginalized over $\Omega_m$. The parentheses near the descriptor label refer to the number of principal components.}
\label{contours3singleRep}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.45]{Figures/contoursSigma8Om055_single.eps}
\end{center}
\caption{$\Sigma_8$ parameter likelihood calculated from the CFHTLenS data using the Power Spectrum (red), $V_0$ (blue), $V_1$ (green), $V_2$ (black) and Moments (orange) statistics; the likelihood has been marginalized over $\Omega_m$ and $w$. The parentheses near the descriptor label refer to the number of principal components.}
\label{likelihoodSi8single}
\end{figure}

\begin{table*}
\begin{tabular}{c|c|c||c}
Parameters & Descriptors & Short description & Relevant Figures \\ \hline \hline
$(\Omega_m,\sigma_8)$ & PS,$V_0,V_1,V_2$,LM & Stability of contours & \ref{robustnessfig} \\ \hline 
$(\Omega_m,\sigma_8)$ & PS(3),$V_0(5),V_1(20),V_2(20)$,LM(9) &\pbox{20cm}{$1\sigma$ constraints from CFHTLenS \\ and mock observations}  & \ref{contours3single},\ref{contours3single}b \\ \hline
$(\Omega_m,\sigma_8)$ & $(\sigma_i^2,S_i,K_i)$ & \pbox{20cm}{$1\sigma$ constraints from CFHTLenS \\ using $\kappa$ Moments \\ combined ad different $\theta_G$}  & \ref{contoursMoments} \\ \hline
$(w,\Sigma_8)$ & PS(3),$V_0(10),V_1(10),V_2(10)$,LM(9) & contours from CFHTLenS & \ref{contours3singleRep} \\ \hline 
$\Sigma_8$ & PS(3),$V_0(10),V_1(10),V_2(10)$,LM(9) & $\mathcal{L}(\Sigma_8)$ from CFHTLenS & \ref{likelihoodSi8single} \\ \hline
$(\Omega_m,\sigma_8)$ & PS(3)$\times V_0(5)\times V_1(20)\times V_2(20)\times$LM(9) & \pbox{20cm}{constraints from CFHTLenS \\ combining statistics} & \ref{contours3combined} \\ \hline
$(w,\Sigma_8)$ & PS(3)$\times V_0(10)\times V_1(10)\times V_2(10)\times$LM(9) & \pbox{20cm}{constraints from CFHTLenS \\ combining statistics} & \ref{contours3combined}b \\ \hline 
$\Sigma_8$ & PS(3)$\times V_0(10)\times V_1(10)\times V_2(10)\times$LM(9) & \pbox{20cm}{$\mathcal{L}(\Sigma_8)$ from CFHTLenS \\ combining statistics} & \ref{likelihoodSi8cross} \\ \hline
\end{tabular}
\caption{Summary table of our results}
\label{summarytable}
\end{table*}
%

\begin{figure*}
\begin{center}
\includegraphics[scale=0.45]{Figures/contoursOmega_m-sigma8_cross.eps}
\includegraphics[scale=0.45]{Figures/contoursw-Sigma8Om055_cross.eps}
\end{center}
\caption{Combined $1\sigma$ constraints on the $(\Omega_m,\sigma_8)$ (left panel) and $(w,\Sigma_8)$ (right panel) doublets, using the PS (red), PS$\times$Moments (green), MFs (blue), MFs$\times$Power Spectrum (black) and MFs$\times$Power Spectrum$\times$Moments (orange). The likelihood function has been marginalized over $w$ (left panel) and $\Omega_m$ (right panel). The parentheses near the descriptor labels refer to the number of principal components.}
\label{contours3combined}
\end{figure*}

\begin{figure}
\begin{center}
\includegraphics[scale=0.45]{Figures/contoursSigma8Om055_cross.eps}
\end{center}
\caption{$\Sigma_8$ parameter likelihood calculated from the CFHTLenS data using the PS (red), PS$\times$Moments (green), MFs (blue), MFs$\times$Power Spectrum (black) and MFs$\times$Power Spectrum$\times$Moments (orange); the likelihood function has been marginalized over $\Omega_m$ and $w$. The parentheses near the descriptor label refer to the number of principal components.}
\label{likelihoodSi8cross}
\end{figure}

\subsection{Combining statistics}
An important point of this work is trying to address is whether the combination of different statistics can help in tighening the constraints on cosmology; some work in this direction has already been done in our companion paper \citep{Companion}, where we combine the power spectrum and peak count statistics, as well as in the literature (see \citep{CFHTFu}) where the authors combine quadratic statistics with the skewness of the CFHTLenS $\kappa$ fields. The procedure we adopt is the following: suppose we consider two descriptors, $d_{1,i},d_{2,i}$ where the index $i$ corresponds to the bin number of the particular descriptor; we compute the single descriptor constraints as in Figure \ref{robustnessfig} and we decide which is the minimum number of principal components $n_{1,2}$ to keep in order for the constraints to be stable. We then construct the vector $d_{1\times2} = (d_1(n_1),d_2(n_2))$ and consider this as the combined feature vector: this procedure allows us to take the cross covariance between different descriptors. We show in Figure \ref{contours3combined} the combined CFHTLenS constraints on the $(\Omega_m,\sigma_8)$ and $(w,\Sigma_8)$ planes, and in Figure \ref{likelihoodSi8cross} the combined $\Sigma_8$ likelihood. We discuss our findings in the next section. 

%AP: Added the statistics combinations Zoltan suggested

%%%%%%%%%%%%%%%%%%%%%%%%%% DISCUSSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{discussion}

In this section we discuss the results outlined in section \S~\ref{results}, with particular focus on the constraints on cosmology. As pointed out in \S~\ref{pcasection}, the choice of the number of bins, $N_b$, is an important point on which one should be careful; in order for our results to be insensitive to $N_b$, we adopt a PCA projection technique to reduce the dimensionality of our feature spaces. In the left panel of Figure \ref{pcafig} we see that the PCA eigenvalues for our descriptors decrease about 4 orders of magnitude from $n=1$ to $n=3$, and looking at the right panel we see that we capture more than 99\% of the feature variance even if we cut the PCA projection to the first 3 components. This does not mean that all the cosmological information is captured by the first 3 PCA components, and this is why we make a stability check by plotting the $1\sigma$ contour sizes as a function of $n$ in Figure \ref{robustnessfig}. Looking at the plot we can observe that, while the first 3 components capture essentially all the information contained in the Power Spectrum, this is not true for the remaining set of statistics. In particular we find that at least 5 components are necessary for $V_0$, and at least 20 components for $V_1,V_2$, in order for the $(\Omega_m,\sigma_8)$ contours to be stable at a $\sim$5\% level. All the 9 components are necessary for the Moments contours to be stable. Matters are slightly different when we study the $(w,\Sigma_8)$ constraints: in this case we find that the optimal choice for the three MFs, $(V_0,V_1,V_2)$, is $n=10$, while the Power Spectrum and Moments components can be kept fixed, at respectively 3 and 9. 

We now discuss the main scientific findings of this work. In Figure \ref{contours3single} we show the $1\sigma$ constraints on the $(\Omega_m,\sigma_8)$ doublet from the CFHTLenS data: we can see that the MFs constraints are heavily biased towards the low--$\sigma_8$, high--$\Omega_m$ region. We believe that this is due to uncorrected systematics in the CFHTLenS data, since when we try to constrain mock observations based on simulations (shown in the left panel of Figure \ref{contours3single}), we recover the correct position of the $1\sigma$ contours. It is important to say, however, that the mock observation to which the right panel of Figure \ref{contours3single} refers, was build with the mean of $R=1000$ realizations of the CFHTcov simulations. We found that it is possible to find some realizations for which the best fit for $(\Omega_m,\sigma_8)$ lies in the lower right corner,even if the likelihood of this happening is very small ($\lesssim1\%$). 

We observe that the Moments give the best constraint on $(\Omega_m,\sigma_8)$; in order to understand what are the features responsible for such tight bounds, we studied the individual contributions of each of the moments to the constraining power. Figure \ref{contoursMoments} shows the evolution of the $(\Omega_m,\sigma_8)$ constraints as we add higer order moments to the feature set: because we are focusing on 3 parameters, we start by considering the set of the three one--point moments $(\sigma_0^2,S_0,K_0)$ and then we add the remaining six moments of gradients one by one, starting from the quadratic ones. We see that the biggest improvement on the parameter bounds comes from including quartic moments of gradients (i.e. $K_i$ with $i\ge1$) in the feature set. This might explain why \citep{CFHTFu} find only unimpressive ($\sim10\%$) contour tightening when adding the skewness to quadratic statistics, since the main improvement comes from higer moments of $\kappa$ derivatives. This result agrees with what \citep{BhuvKurtosis} in their previous work, in which they state that the kurtosis of the shear field can help in breaking degeneracies between $\Omega_m$ and $\sigma_8$. We also observe that the improvement we obtain combining the one--point moments at different smoothing scales $\theta_G$, is not as good as the one that we get considering the moments of gradients.      

The bias in the $(\Omega_m,\sigma_8)$ constraints is made worse by the cosmological degeneracy of these parameters; to mitigate this effect, we consider the combination of $\Omega_m$ and $\sigma_8$ that lies along the non degenerate direction, namely $\Sigma_8=\sigma_8(\Omega_m/0.27)^{0.55}$. In Figure \ref{likelihoodSi8single} we show the $1\sigma$ constraints on the $(w,\Sigma_8)$ doublet, as well as the marginalized $\Sigma_8$ likelihood from the CFHTLenS data: we can see that this survey is not sufficient to constrain $w$, but on the other hand it does a good job in nailing the $\Sigma_8$ combination to a value of $\Sigma_8=0.75\pm0.04(1\sigma)$, using either of the MFs. The numerical value that we find for $\Sigma_8$ agrees with the one previously published by the CFHTLenS collaboration in \citep{CFHTKilbinger}. Regarding the parameter biases, our results are in accordance with what already found in \citep{PetriSpurious}, who found that unaccounted systematics result in larger parameter biases when the contstraints come from the MFs. In accordance with the same work, we found that the LM statistic is the less biased one and hence is the ideal tool to detect cosmological information in CFHTLenS. 

We next studied whether the combination of different statistical descriptors can help in tightening the cosmological constraints; we show the effects of some of these combinations in Figures \ref{contours3combined} and \ref{likelihoodSi8cross}. The left panel of Figure \ref{contours3combined} shows that, although combining the Power Spectrum with the Minkowski Functionals and Moments helps in shrinking the $(\Omega_m,\sigma_8)$ constraints, unfortunately this does not help in reducing the inherent parameter bias. In the right panel of Figure \ref{contours3combined} we can see that even with these statistics combinations, $w$ remains essentially unconstrained. Figure \ref{likelihoodSi8cross} shows that the $\Sigma_8$ combination is already well constrained with either the MFs alone, without the need of combining different descriptors.   

\newpage 
%%%%%%%%%%%%%%%%%%%%%%%%%% CONCLUSIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

In this final section we summarize the main conclusions of this work:
\begin{itemize}
\item When constraining the $(\Omega_m,\sigma_8)$ doublet with the CFHTLenS survey, we find that the Moments of the $\kappa$ field provide the tighest constraint on the former parameters; this constraint has also the advantage of being unbiased. We found that the biggest improvement on these parameter bounds is achieved when we include the quartic moments of gradients in the feature set; this improvement cannot be obtained by combining one--point moments at different smoothing scales 
\item Although weak lensing surveys are a promising technique to constrain the DE equation of state $w$, reasonable constraints cannot be obtained with the CFHTLenS survey alone, even when using additional sets of descriptors that go beyond the standard quadratic statistics.
\item When studying the cosmological information contained in the CFHTLenS data, one must pay special attention to the correction of residual systematics; while it seems these residual systematics are not important when constraining cosmology with the Power Spectrum alone, we find that these systematics need to be corrected for to obtain sensible constraints on the $(\Omega_m,\sigma_8)$ doublet using the Minkowski Functionals. 
\item The Minkowski Functionals alone are sufficient to constrain the $\Sigma_8$ combination to a value of $\Sigma_8=0.75\pm0.04$ at $1\sigma$ significance level; this agrees with the value previously published by the CFHTLenS collaboration within $1\sigma$. Some tensions with Planck \citep{PlanckXVI2013} still remain.  
\end{itemize}

Possible future developments of this work include simulating higher dimensional parameter spaces (possibly including varying Hubble parameter $H_0$ and the running of the DE equation of state $w_1$), and combining constraints from different cosmological probes (data from Planck for example), that can help in breaking the $\Omega_m,\sigma_8$ degeneracy. 

%%%%%%%%%%%%%%%%%%%%%%%%%% ACKNOWLEDGMENTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 

\section*{Acknowledgements}
We thank the Stampede cluster who saved our day

\bibliography{ref}
\label{lastpage}
\end{document}
